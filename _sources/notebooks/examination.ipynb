{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e80f87",
   "metadata": {},
   "source": [
    "# Home examination, May 2023\n",
    "Follow instructions on the course's Moodle workspace on running and submitting the completed notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da811cf9",
   "metadata": {},
   "source": [
    "## <u> Task 0: import libraries, set environment properties, get and set data </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f73cd7",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ae640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47901897",
   "metadata": {},
   "source": [
    "### Set environment properties (may be adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = [10, 10] # controls the size of produced figures, adjust if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befef12a",
   "metadata": {},
   "source": [
    "### Get and set data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# assign dataset data to variables used in analysis\n",
    "feature_data = iris.data\n",
    "feature_labels = iris.target\n",
    "feature_names = iris.feature_names\n",
    "feature_label_names = iris.target_names\n",
    "\n",
    "# create a pandas dataframe to store the data, as well\n",
    "[num_examples, num_data_dimensions] = feature_data.shape\n",
    "df = pd.DataFrame(data=feature_data, columns=feature_names)\n",
    "\n",
    "# insert also label data, the information about from which Iris \n",
    "# subspecies are the feature measurements, \n",
    "# given by the data point elements, from.\n",
    "df = pd.DataFrame(data=feature_data, columns=feature_names)\n",
    "df.insert(0, \"subspecies\", feature_labels, True)\n",
    "\n",
    "# use actual subspecies names instead off number codes\n",
    "for ii in np.arange(feature_label_names.size):\n",
    "    df.replace({'subspecies': ii}, feature_label_names[ii], inplace=True)\n",
    "\n",
    "# display the dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeea398",
   "metadata": {},
   "source": [
    "## <u> Task 1: Scatter plotting and histogramming data, analysis based on them </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54efc4f",
   "metadata": {},
   "source": [
    "###  Helper functions and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c849ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_estimation_of_univariate_data_density(data, n_bins=None, min_edge=None, max_edge=None):\n",
    "    \"\"\"\n",
    "    Calculates a normalized histogram of data, to get an approximate density representation.\n",
    "    Inputs:\n",
    "        - data: data\n",
    "        - n_bins: amount of histogram bins to use\n",
    "        - min_edge: smallest considered value in data\n",
    "        - max_edge: largest considered value in data \n",
    "    Outputs:\n",
    "        - bin_centers: centers of the bins\n",
    "        - normalized_bin_counts: normalized bin counts, approximate densities\n",
    "    \"\"\"\n",
    "    if n_bins is None:\n",
    "        n_bins = int(np.sqrt(data.size))\n",
    "    if min_edge is None or max_edge is None:\n",
    "        data_min = data.min()\n",
    "        data_max = data.max()\n",
    "        data_range = data_max-data_min\n",
    "    if min_edge is None:\n",
    "        min_edge = data_min-0.1*data_range\n",
    "    if max_edge is None:\n",
    "        max_edge = data_max+0.1*data_range\n",
    "\n",
    "    # specify bins\n",
    "    bin_edges = np.linspace(min_edge, max_edge, n_bins+1)\n",
    "    bin_centers = 0.5*(bin_edges[0:-1]+bin_edges[1:])\n",
    "\n",
    "    # calculate a normalized histogram\n",
    "    [normalized_bin_counts, _] = np.histogram(data, bin_edges, density=True)\n",
    "\n",
    "    return bin_centers, normalized_bin_counts, bin_edges\n",
    "\n",
    "\n",
    "def estimate_and_plot_histogram(data, ax, title, n_bins=None, \n",
    "                                data_classes=None, class_names=None, class_colours=None):\n",
    "    if data_classes is None:\n",
    "        bin_centers, normalized_bin_counts, _ = histogram_estimation_of_univariate_data_density(data, n_bins)\n",
    "        if n_bins is None:\n",
    "            n_bins=bin_centers.size\n",
    "        bin_width = bin_centers[1]-bin_centers[0]\n",
    "        ax.bar(bin_centers, normalized_bin_counts, width=bin_width, color='blue', \n",
    "           edgecolor='black' if n_bins<= 100 else 'blue', alpha=0.5)\n",
    "        ax.set_title(title)\n",
    "        ax.set_yticks([]);\n",
    "    else:\n",
    "        num_classes = len(class_names)\n",
    "        bin_centers, normalized_bin_counts, bin_edges = histogram_estimation_of_univariate_data_density(data, n_bins)\n",
    "        for class_index, class_name in enumerate(class_names):\n",
    "            [normalized_bin_counts, _] = np.histogram(data[data_classes==class_name], bin_edges, density=True)\n",
    "            ax.plot(bin_centers, normalized_bin_counts, color=class_colours[class_index, :], linewidth=1.5, alpha=0.9)\n",
    "        ax.set_title(title)\n",
    "        ax.set_yticks([]);\n",
    "\n",
    "        \n",
    "def estimate_and_plot_marginal_densities(df, feature_names, n_bins=None, \n",
    "                                         data_classes=None, class_names=None, class_colours=None):\n",
    "    n_features = len(feature_names)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=n_features, figsize=[FIGSIZE[0], FIGSIZE[0]/n_features])\n",
    "    for feature_index, feature_name in enumerate(feature_names):\n",
    "        estimate_and_plot_histogram(df[feature_names[feature_index]].to_numpy(), ax[feature_index], \n",
    "                                    feature_names[feature_index], n_bins, data_classes, class_names, class_colours)\n",
    "    if class_names is not None:\n",
    "        ax[-1].legend(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81759afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a color map used in plotting\n",
    "color_palette = sns.color_palette(as_cmap=True)\n",
    "feature_label_marker_colors = np.zeros((3, 3))\n",
    "feature_label_marker_colors[0, :] = colors.to_rgb(color_palette[0])\n",
    "feature_label_marker_colors[1, :] = colors.to_rgb(color_palette[1])\n",
    "feature_label_marker_colors[2, :] = colors.to_rgb(color_palette[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafe829",
   "metadata": {},
   "source": [
    "## Task 1 A.\n",
    "\n",
    "Use seaborn's pairplot-function to look at pairwise depencencies of the features (via scatter plots of feature pairs), and marginal distributions of features (via histograms of features). Use also the histogram estimation as given by the created estimate_and_plot_marginal_densities-function, to investigate the marginal distributions. \n",
    "\n",
    "In the 'Answer'-section below, answer to the the following questions: \n",
    "\n",
    "Distribution properties: \n",
    "   * do the estimated marginal distributions appear to be unimodal? \n",
    "   * does the data appear normally distributed, marginally, jointly?\n",
    "\n",
    "Amount of bins: \n",
    "   * does the amount of bins used by seaborn.pairplot seem reasonable? \n",
    "   * are the results sensitive to the the amount of bins used (this can varied in the estimate_and_plot_marginal_densities-function)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = None # if None, automatic setting based on the amount of datapoints\n",
    "estimate_and_plot_marginal_densities(df, feature_names, n_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e163f0",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb768aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a35965d",
   "metadata": {},
   "source": [
    "## Task 1 B.\n",
    "\n",
    "Now let's utilize label information, the iris subspecies information, to understand the data better. Utilize the seaborn.pairplot and the created estimate_and_plot_marginal_densities-function in the analysis. In doing marginal density \n",
    "estimation, also compare the kernel density estimates by seaborn.pairplot to the histogram-based density estimate curves by the \n",
    "estimate_and_plot_marginal_densities-function.\n",
    "\n",
    "In the 'Answer'-section below, answer to the following questions: \n",
    "* Do the class-specific marginal distributions appear unimodal?\n",
    "* Are the class-specific marginal distributions appear more normally distributed than the class-insensitive distributions?\n",
    "* Does the amount of class-specific samples appear to be sufficient to construct reasonable histograms? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"subspecies\", diag_kind=\"kde\", diag_kws={'fill': False});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6851a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_and_plot_marginal_densities(df, feature_names, n_bins=None, data_classes=df['subspecies'], \n",
    "                                     class_names=feature_label_names, class_colours=feature_label_marker_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c14848",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25356b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33068c7e",
   "metadata": {},
   "source": [
    "## <u> Task 2: Dimensionality reduction, and assessment </u>\n",
    "Here we do principal components analysis (PCA) [see e.g., https://en.wikipedia.org/wiki/Principal_component_analysis] to do dimensionality reduction. The data projection it uses aims, e.g., to project data such that (variation in) the data can be described using less dimensions than in the original space. The approach provides mechanisms to project data onto a lower dimension (to the principal components), and also to reconstruct from the lower dimension back to the original space but potentially with error; the projections are linear/affine. \n",
    "\n",
    "Basically, a coordinate value of a datapoint in the projected (lower-dimensional) space, is obtained by a linear combination (weighted sum) of the coordinate values in the original space subtracted by the mean coordinate values in the original space; for each principal component coordinate, the combination weights are different, and e.g. the first principle component weights give the direction of highest variance in the data, the second component the direction of second highest variance that is orthogonal to the direction of the first one, the third the direction of third highest variance that is orthogonal to the previous directions, and so on. The reconstruction of a coordinate value from the projection space, is obtained by adding to the mean coordinate value in the original space, a linear combination of the coordinate values in the projection space (the principal components space); for each original space coordinate, the combination weights are different. \n",
    "The projection/combination weights can be estimated from the sample covariance matrix, via an eigendecomposition; the eigenvector associated with highest eigenvalue gives the first principle component, the eigenvector associated with the second highest eigenvalue gives the second principle component, and so on; the eigenvalues give the data variance in the direction of the component. \n",
    "\n",
    "First estimate a PCA-basis from the data. Then study the effect of the amount of principal components used, to: \n",
    "* the quality of reconstruction from the projection\n",
    "* the class separability in the projection space\n",
    "* the easiness of understanding the phenomena in the data. \n",
    "\n",
    "Do them by executinh the cells below in this section; to change the amount of principle components used, change the value of the \n",
    "'num_pcs'-variable (line between the two lines containing comment 'CHANGE CODE'). \n",
    "\n",
    "In the 'Answer'-section, answer briefly to the following questions:\n",
    "* Would we lose a lot of information by using 3 principal components? \n",
    "* Would classification performance appear to be effected hugely by considering 2 instead of 3 principal components?\n",
    "* Could we reconstruct well from 2 principal components? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d4ce1",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_principal_component_analysis_basis(data):\n",
    "    # calculate sample mean of data\n",
    "    data_center = np.sum(data, axis=0, keepdims=True)/data.shape[0]\n",
    "    # center data by subtracting the mean\n",
    "    centered_data = data-data_center \n",
    "    # calculate covariance matrix\n",
    "    sample_covariance = np.dot(centered_data.transpose(), centered_data)/(data.shape[0]-1)\n",
    "    # eigendecompose covariance matrix to get the principal components\n",
    "    principal_component_variances, principal_components = np.linalg.eig(sample_covariance)    \n",
    "    return data_center, principal_component_variances, principal_components\n",
    "\n",
    "\n",
    "def project_data_onto_principal_components(data, data_center, principal_components, num_pcs):\n",
    "    projected_data = np.dot(data-data_center, principal_components[:, 0:num_pcs])\n",
    "    return projected_data\n",
    "\n",
    "\n",
    "def reconstruct_from_principal_components(projected_data, data_center, principal_components, num_pcs):\n",
    "    reconstructed_data = np.dot(projected_data, principal_components[:, 0:num_pcs].transpose())+data_center\n",
    "    return reconstructed_data\n",
    "\n",
    "\n",
    "def assess_reconstruction_error_numerically(data, reconstructed_data):\n",
    "    # mean squared-error, MSE, is used as the error metric\n",
    "    reconstruction_error = np.mean(np.sum((data-reconstructed_data)**2, axis=1))\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f1518",
   "metadata": {},
   "source": [
    "### PCA-basis estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_center, principal_component_variances, principal_components = estimate_principal_component_analysis_basis(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c756906",
   "metadata": {},
   "source": [
    "### Analysis of the use of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fa71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d41a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# ----------> CHANGE CODE: VARY THIS, THE AMOUNT OF PRINCIPLE COMPONENTS TO USE\n",
    "num_pcs = 4\n",
    "# <---------- CHANGE CODE\n",
    "#################################################################################\n",
    "\n",
    "# data projecting\n",
    "# -----------------\n",
    "\n",
    "# project data onto PCA-space\n",
    "projected_data = project_data_onto_principal_components(feature_data, data_center, principal_components, num_pcs)\n",
    "# reconstruct from the PCA-space\n",
    "reconstructed_data = reconstruct_from_principal_components(projected_data, data_center, principal_components, num_pcs)\n",
    "\n",
    "# assess performance quantitatively \n",
    "# ---------------------------------\n",
    "reconstruction_error = assess_reconstruction_error_numerically(feature_data, reconstructed_data)    \n",
    "\n",
    "# assess performance qualitatively\n",
    "# ---------------------------------\n",
    "\n",
    "# create a dataframe to \"pairplot\" reconstructed data\n",
    "df2 = pd.DataFrame(data=reconstructed_data, columns=feature_names)\n",
    "df2.insert(0, \"subspecies\", feature_labels, True)\n",
    "for ii in np.arange(feature_label_names.size):\n",
    "    df2.replace({'subspecies': ii}, feature_label_names[ii], inplace=True)\n",
    "\n",
    "# create a dataframe to \"pairplot\" projected data \n",
    "df3 = pd.DataFrame(data=projected_data, columns=[f'PC{pc_index}' for pc_index in np.arange(1, num_pcs+1)])\n",
    "df3.insert(0, \"subspecies\", feature_labels, True)\n",
    "for ii in np.arange(feature_label_names.size):\n",
    "    df3.replace({'subspecies': ii}, feature_label_names[ii], inplace=True)\n",
    "\n",
    "# \"pairplot\" reconstructions\n",
    "pairplot_reconstructed = sns.pairplot(df2, hue=\"subspecies\", diag_kind=\"kde\", diag_kws={'fill': False})\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle(f'num pcs: {num_pcs}, reconstruction error: {reconstruction_error}')\n",
    "\n",
    "\n",
    "# \"pairplot\" original data\n",
    "pairplot_original = sns.pairplot(df, hue=\"subspecies\", diag_kind=\"kde\", diag_kws={'fill': False})\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle('Original')\n",
    "\n",
    "# \"pairplot\" projection\n",
    "pairplot_projection = sns.pairplot(df3, hue=\"subspecies\", diag_kind=\"kde\", diag_kws={'fill': False})\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle('Projection');\n",
    "\n",
    "# scatter plot also the projection if we use 3 or 2 dimensions, \n",
    "if num_pcs < 4:\n",
    "    if num_pcs == 3:\n",
    "        fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=[10, 10])\n",
    "        for species_index, species_label in enumerate(feature_label_names):\n",
    "            species_rows = feature_labels == species_index\n",
    "            ax.scatter(projected_data[species_rows, 0], projected_data[species_rows, 1], projected_data[species_rows, 2], \n",
    "                       c = feature_label_marker_colors[species_index:species_index+1, :], \n",
    "                       alpha=0.5, s=100, label=species_label)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_zlabel('PC3')\n",
    "        ax.set_title('Projected data; click the pan-button to be able to change the viewpoint & zoom');\n",
    "    elif num_pcs == 2:\n",
    "        fig, ax = plt.subplots(figsize=[10, 10])\n",
    "        for species_index, species_label in enumerate(feature_label_names):\n",
    "            species_rows = feature_labels == species_index\n",
    "            ax.scatter(projected_data[species_rows, 0], projected_data[species_rows, 1], \n",
    "                     c = feature_label_marker_colors[species_index:species_index+1, :], \n",
    "                       alpha=0.5, s=100, label=species_label)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_title('Projected data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56797a5f",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf2494",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9c8b93f",
   "metadata": {},
   "source": [
    "## <u> Task 3: Density estimation of projected data, modeling effectiveness assessment </u>\n",
    "Here, we model data given by a PCA-projection of the original data, using 2 (of the first) principal components, and do a bit of modeling effectiveness assessment. The modeling is by fitting class-specific (Iris subspecies -specific) multivariate normal distributions. The modeling effectiveness assessment here is by qualitative assessment, by observing and analyzing the information in the created plots. \n",
    "\n",
    "The codes below provide the functionality for doing the model fitting also also for creating the plots. One of your tasks\n",
    "is to modify the 'estimate_normal_parameters'-function below (in section 'Definition of helper functions') that estimates and outputs sample mean vector and sample covariance matrix from its input data: you need to replace the implementation that is provided (lines between the two lines containing comment 'CHANGE CODE'), by a different one that is your own; you still need to output sample mean and sample covariance. The other task is to do the effectiveness assessment as mentioned above.\n",
    "\n",
    "In the 'Answer'-section, briefly describe i) your code changes, ii) the qualitative assessment of modeling effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7631e",
   "metadata": {},
   "source": [
    "### Data projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1799d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd37e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pcs = 2\n",
    "projected_data = project_data_onto_principal_components(feature_data, data_center, principal_components, num_pcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056b124",
   "metadata": {},
   "source": [
    "### Definitions of helper functions\n",
    "You need to modify the estimate_normal_parameters-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2D_gaussian_equiprobability_points(mean_vector, covariance_matrix, num_points = 100):\n",
    "    '''\n",
    "    Gets equiprobable random variable values under a bivariate normal; under \n",
    "    a standard bivariate normal the points away from the mean along a dimension/axis are a \n",
    "    standard deviation away.\n",
    "    \n",
    "    inputs:\n",
    "    -------\n",
    "    mean_vector: 1 x 2 matrix (/row vector),  \n",
    "    covariance_matrix: 2 x 2 matrix\n",
    "    num_points: integer specifying the amount of points to compute\n",
    "    \n",
    "    outputs:\n",
    "    --------\n",
    "    positions: num_points x 2 matrix; the positions of the points\n",
    "    '''\n",
    "    [eigenvalues, eigenvectors] = np.linalg.eig(covariance_matrix)\n",
    "    angle = np.linspace(0, 2*np.pi, num_points)\n",
    "    positions_standard = np.empty((num_points, 2)) \n",
    "    positions_standard[:, 0] = np.cos(angle) \n",
    "    positions_standard[:, 1] = np.sin(angle)\n",
    "    positions = np.dot(positions_standard, np.dot(eigenvectors, np.diag(np.sqrt(eigenvalues))).transpose())+mean_vector\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_normal_parameters(subspecies_data):\n",
    "    \"\"\"\n",
    "    Estimates parameters of a multivariate normal distribution. \n",
    "    \n",
    "    Input:\n",
    "        - subspecies data: an array of the subspecies data, rows index examples, columns index dimensions\n",
    "    Output:\n",
    "        - subspecies_mean: sample mean of subspecies data \n",
    "        - subspecies_covariance: sample covariance matrix of subspecies data\n",
    "    \"\"\"\n",
    "    #########################################################################################\n",
    "    # ----------> CHANGE CODE: REPLACE WITH YOUR OWN, DIFFERENT IMPLEMENTATION\n",
    "    num_subspecies_examples = subspecies_data.shape[0]\n",
    "    subspecies_mean = np.sum(subspecies_data, axis=0, keepdims=True)/num_subspecies_examples\n",
    "    centered_subspecies_points = subspecies_data-subspecies_mean\n",
    "    subspecies_covariance = np.dot(centered_subspecies_points.transpose(), \n",
    "                                   centered_subspecies_points)/(num_subspecies_examples-1)\n",
    "    # < --------- CHANGE CODE \n",
    "    #########################################################################################\n",
    "    \n",
    "    return subspecies_mean, subspecies_covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47191391",
   "metadata": {},
   "source": [
    "### Model parameter estimation and modeling effectiveness assessment\n",
    "Run and do the qualitative assessment based on the created plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0c6be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_subspecies = feature_label_names.size\n",
    "subspecies_means = np.empty((num_subspecies, 2), dtype=float)\n",
    "subspecies_covariances = np.empty((2, 2, num_subspecies), dtype=float)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=[FIGSIZE[0], 2*FIGSIZE[1]])\n",
    "\n",
    "# original data\n",
    "for subspecies_index, subspecies_label in enumerate(feature_label_names):\n",
    "    subspecies_rows = feature_labels == subspecies_index\n",
    "    subspecies_mean, subspecies_covariance = estimate_normal_parameters(projected_data[subspecies_rows, :])        \n",
    "    subspecies_means[subspecies_index, :] = subspecies_mean.copy()\n",
    "    subspecies_covariances[:, :, subspecies_index] = subspecies_covariance.copy()\n",
    "    equiprobability_points = get_2D_gaussian_equiprobability_points(subspecies_mean, subspecies_covariance)\n",
    "    ax1.plot(equiprobability_points[:, 0], equiprobability_points[:, 1], \n",
    "             color=feature_label_marker_colors[subspecies_index:subspecies_index+1, :], \n",
    "             alpha=0.5, linestyle='dashed', label=subspecies_label+'-gaussian')    \n",
    "    probabilities = stats.multivariate_normal.pdf(projected_data[subspecies_rows, :], \n",
    "                                                  mean=subspecies_mean.flatten(), \n",
    "                                                  cov=subspecies_covariance)\n",
    "    ax1.scatter(projected_data[subspecies_rows, 0], projected_data[subspecies_rows, 1],  \n",
    "                c = feature_label_marker_colors[subspecies_index:subspecies_index+1, :], \n",
    "                alpha=0.1+0.8*probabilities/probabilities.max(), s=100, label=subspecies_label);\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('projection dimension 1'); ax1.set_ylabel('projection dimension 2');\n",
    "ax1.set_title('Projected data modeled with class-specific multivariate normal distributions; \\n strength of colour encodes density value, density values on the ellipse points are equal')\n",
    "xlim = ax1.get_xlim()\n",
    "ylim = ax1.get_ylim()\n",
    "\n",
    "# samples from the model, i.e. fake data \n",
    "for subspecies_index, subspecies_label in enumerate(feature_label_names):\n",
    "    num_random_samples_per_class = int(np.sum(feature_labels == subspecies_index))\n",
    "    subspecies_mean = subspecies_means[subspecies_index, :].copy()\n",
    "    subspecies_covariance = subspecies_covariances[:, :, subspecies_index].copy()\n",
    "    subspecies_fakedata = stats.multivariate_normal.rvs(mean=subspecies_mean, cov=subspecies_covariance,\n",
    "                                  size=num_random_samples_per_class)\n",
    "    probabilities_fakedata = stats.multivariate_normal.pdf(subspecies_fakedata, \n",
    "                                                           mean=subspecies_mean, cov=subspecies_covariance)\n",
    "    ax2.scatter(subspecies_fakedata[:, 0], subspecies_fakedata[:, 1],  \n",
    "                c = feature_label_marker_colors[subspecies_index:subspecies_index+1, :], \n",
    "                alpha=0.1+0.8*probabilities_fakedata/probabilities_fakedata.max(), s=100, label=subspecies_label);\n",
    "    equiprobability_points = get_2D_gaussian_equiprobability_points(subspecies_mean, subspecies_covariance)\n",
    "    ax2.plot(equiprobability_points[:, 0], equiprobability_points[:, 1], \n",
    "             color=feature_label_marker_colors[subspecies_index:subspecies_index+1, :], \n",
    "             alpha=0.5, linestyle='dashed', label=subspecies_label+'-gaussian')    \n",
    "ax2.legend()\n",
    "ax2.set_xlabel('projection dimension 1')\n",
    "ax2.set_ylabel('projection dimension 2')\n",
    "ax2.set_title('Samples from the model/fake data');\n",
    "ax2.set_xlim(xlim)\n",
    "ax2.set_ylim(ylim);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d775e6",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add00f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7066e5f",
   "metadata": {},
   "source": [
    "## <u> Task 4: Classifying projected data </u>\n",
    "We first classify the projected data using a Bayes-classifier utilizing the Iris subspecies -specific multivariate Gaussian distributions we fitted in the previous task, and analyze the effectiveness. Probably there will be classification errors.\n",
    "\n",
    "We then consider a setting, where the classifier refuses to classify a datapoint if the uncertainty about the class is too high; the uncertainty shall be measured via entropy of the distribution of predicted class probabilities, for the datapoint. This setting is also done with a different classifier model, a multinomial logistic regression model; multinomial logistic regression generalizes logistic regression for multi-class classification problems.\n",
    "\n",
    "Run the analyses, with varying uncertainty thresholds; such thresholds are set in two places in the codes below and in each case  they are set between two lines of code containing comment 'CHANGE CODE: VARY THIS'). In this task and in any other tasks, we have not split data onto separate development and testing sets (like splitting the data onto training, validation, and test portions). Think about how the modeling and analysis should be changed under such more proper setting.  \n",
    "\n",
    "In the 'Answer'-section, briefly describe i) the usefulness of the prediction uncertainty quantification, ii) how the modeling, including the threshold setting could be done in a more appropriate way, utilizing the data splitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687f9fa",
   "metadata": {},
   "source": [
    "### Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "# --------------\n",
    "prior_probabilities = np.ones((num_subspecies,))/num_subspecies\n",
    "unnormalized_posterior_probabilities = np.array([prior_probabilities[subspecies_index]*stats.multivariate_normal.pdf(projected_data, mean=subspecies_means[subspecies_index, :], cov=subspecies_covariances[:, :, subspecies_index]) for subspecies_index in np.arange(num_subspecies)])\n",
    "classification_class = np.argmax(unnormalized_posterior_probabilities, axis=0)\n",
    "\n",
    "# Scatter plotting to visualize classifications \n",
    "# ----------------------------------------------\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=[FIGSIZE[0], 2*FIGSIZE[1]])\n",
    "for subspecies_index, subspecies_label in enumerate(feature_label_names):\n",
    "    subspecies_rows = feature_labels == subspecies_index\n",
    "    num_subspecies_examples = np.sum(subspecies_rows)\n",
    "    ax1.scatter(projected_data[subspecies_rows, 0], projected_data[subspecies_rows, 1],\n",
    "                c = feature_label_marker_colors[subspecies_index:subspecies_index+1, :],\n",
    "                alpha=0.5, s=100, label=subspecies_label,\n",
    "                edgecolors = feature_label_marker_colors[classification_class[subspecies_rows], :])\n",
    "    subspecies_mean = subspecies_means[subspecies_index, :].copy()\n",
    "    subspecies_covariance = subspecies_covariances[:, :, subspecies_index].copy()\n",
    "    equiprobability_points = get_2D_gaussian_equiprobability_points(subspecies_mean, subspecies_covariance)\n",
    "    ax1.plot(equiprobability_points[:, 0], equiprobability_points[:, 1], \n",
    "             color=feature_label_marker_colors[subspecies_index:subspecies_index+1, :], \n",
    "             alpha=0.5, linestyle='dashed', label=subspecies_label+'-gaussian')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('projection dimension 1')\n",
    "ax1.set_ylabel('projection dimension 2')\n",
    "ax1.set_title('Projected data classified; marker edge colour gives the predicted class; marker fill colour gives the true class')\n",
    "\n",
    "# Confusion matrix to visualize classifications\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_matrix = np.empty((num_subspecies, num_subspecies), dtype='int')\n",
    "for true_index in np.arange(num_subspecies):\n",
    "    for classified_index in np.arange(num_subspecies):\n",
    "        confusion_matrix[true_index, classified_index] = np.sum(np.logical_and(feature_labels == true_index, \n",
    "                                                                               classification_class == classified_index))\n",
    "normalized_confusion_matrix = confusion_matrix/np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# Visualize confusion matrix as an image, gray-scale value encodes the numerical value\n",
    "im = ax2.imshow(normalized_confusion_matrix, cmap=plt.cm.gray, interpolation=\"nearest\", extent=[0, 1, 0, 1])\n",
    "ax2.tick_params(top=False, bottom=False, labeltop=True, labelbottom=False, left=False, labelleft=True, right=False)\n",
    "ax2.set_yticks((0.5+np.arange(len(feature_label_names)))/len(feature_label_names), feature_label_names[::-1])\n",
    "ax2.set_xticks((0.5+np.arange(len(feature_label_names)))/len(feature_label_names), \n",
    "               labels=['predicted as '+feature_label_name for feature_label_name in feature_label_names])\n",
    "\n",
    "\n",
    "# Append text annotations to show also the numerical values in textual format\n",
    "for ii, vertical_pos in enumerate(1-(0.5+np.arange(len(feature_label_names)))/len(feature_label_names)):\n",
    "    for jj, horizontal_pos in enumerate((0.5+np.arange(len(feature_label_names)))/len(feature_label_names)):\n",
    "        if normalized_confusion_matrix[ii, jj]>=0.5:\n",
    "            textcolor = 'k'\n",
    "        else:\n",
    "            textcolor = 'w'\n",
    "        ax2.text(horizontal_pos, vertical_pos, confusion_matrix[ii, jj], ha=\"center\", va=\"center\", color=textcolor)\n",
    "ax2.set_title(\"Confusion matrix\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82775d85",
   "metadata": {},
   "source": [
    "### Refusing to classify, when there is too much uncertainty about the prediction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae55768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_based_on_entropy(probabilities_over_classes_over_rows):\n",
    "    return -np.sum(probabilities_over_classes_over_rows*np.log(probabilities_over_classes_over_rows), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e581ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_posterior_probabilities = unnormalized_posterior_probabilities/np.sum(unnormalized_posterior_probabilities, \n",
    "                                                                                 axis=0, keepdims=True)\n",
    "class_uncertainty = get_uncertainty_based_on_entropy(normalized_posterior_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafa95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy of maximally entropic distribution: -\\sum_{c\\in [1,..,C]} (1/C)*log(1/C) = -C*(1/C)*log(1/C)=-log(1/C)=log(C)\n",
    "max_entropy = np.log(num_subspecies) \n",
    "\n",
    "# plot the class uncertainties as measured by the entropies, and also mark the maximum entropy \n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.hist(class_uncertainty[classification_class!=feature_labels], \n",
    "        bins = int(np.sqrt(np.sum(classification_class!=feature_labels))), \n",
    "        color='r', alpha=0.5, label='incorrect classifications');\n",
    "ax.hist(class_uncertainty[classification_class==feature_labels], \n",
    "        bins = int(np.sqrt(np.sum(classification_class==feature_labels))), \n",
    "        color='g', alpha=0.5, label='correct classifications');\n",
    "ylim = ax.get_ylim()\n",
    "ax.vlines(max_entropy, ylim[0], ylim[1], linestyles='dashed', colors=['k'], label='maximally entropic distribution')\n",
    "ax.legend(loc='center')\n",
    "ax.set_title('Entropies of predicted class probabilities');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f97cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_class_color = np.array([[0., 0., 0.]])\n",
    "feature_label_marker_colors = np.concatenate((feature_label_marker_colors, uncertain_class_color), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification \n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "################################################################################################\n",
    "# ----------> CHANGE CODE: VARY THIS\n",
    "entropy_threshold = np.min(class_uncertainty[classification_class!=feature_labels]) # \"cheat\"\n",
    "# <--------- CHANGE CODE: VARY THIS\n",
    "################################################################################################\n",
    "\n",
    "classification_class_adjusted = classification_class.copy()\n",
    "too_uncertain = class_uncertainty >= entropy_threshold \n",
    "classification_class_adjusted[too_uncertain]=num_subspecies\n",
    "\n",
    "# Scatter plotting to visualize classifications \n",
    "# ----------------------------------------------\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=[FIGSIZE[0], 2*FIGSIZE[1]])\n",
    "for subspecies_index, subspecies_label in enumerate(feature_label_names):\n",
    "    subspecies_rows = feature_labels == subspecies_index\n",
    "    num_subspecies_examples = np.sum(subspecies_rows)\n",
    "    ax1.scatter(projected_data[subspecies_rows, 0], projected_data[subspecies_rows, 1],\n",
    "               c = feature_label_marker_colors[subspecies_index:subspecies_index+1, :],\n",
    "               alpha=1-(class_uncertainty[subspecies_rows]/max_entropy), s=100, label=subspecies_label,\n",
    "               edgecolors = feature_label_marker_colors[classification_class_adjusted[subspecies_rows], :]);    \n",
    "    subspecies_mean = subspecies_means[subspecies_index, :].copy()\n",
    "    subspecies_covariance = subspecies_covariances[:, :, subspecies_index].copy()\n",
    "    equiprobability_points = get_2D_gaussian_equiprobability_points(subspecies_mean, subspecies_covariance)\n",
    "    ax1.plot(equiprobability_points[:, 0], equiprobability_points[:, 1], \n",
    "             color=feature_label_marker_colors[subspecies_index:subspecies_index+1, :], \n",
    "             alpha=0.5, linestyle='dashed', label=subspecies_label+'-gaussian')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('projection dimension 1'); ax.set_ylabel('projection dimension 2');\n",
    "ax1.set_title('Projected data classified: \\n colour indicates predicted class, \\n certainty in prediction encoded by colour strength, \\n black edges indicate too much uncertainty and thus refusal to classify');\n",
    "\n",
    "# Confusion matrix to visualize classifications\n",
    "# ----------------------------------------------\n",
    "\n",
    "# calculate confusion matrix\n",
    "confusion_matrix = np.empty((num_subspecies, num_subspecies+1), dtype='int')\n",
    "for true_index in np.arange(num_subspecies):\n",
    "    for classified_index in np.arange(num_subspecies+1):\n",
    "        confusion_matrix[true_index, classified_index] = np.sum(np.logical_and(feature_labels == true_index, classification_class_adjusted == classified_index))\n",
    "normalized_confusion_matrix = confusion_matrix/np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# Visualize confusion matrix as an image, gray-scale value encodes the numerical value\n",
    "im = ax2.imshow(normalized_confusion_matrix, cmap=plt.cm.gray, interpolation=\"nearest\", extent=[0, 1, 0, 1])\n",
    "ax2.tick_params(top=False, bottom=False, labeltop=True, labelbottom=False, left=False, labelleft=True, right=False)\n",
    "ax2.set_yticks((0.5+np.arange(len(feature_label_names)))/len(feature_label_names), feature_label_names[::-1])\n",
    "ax2.set_xticks((0.5+np.arange(len(feature_label_names)+1))/(len(feature_label_names)+1), labels=['predicted as '+feature_label_name for feature_label_name in feature_label_names]+['too uncertain to classify'])\n",
    "\n",
    "# Append text annotations to show also the numerical values in textual format\n",
    "for ii, vertical_pos in enumerate(1-(0.5+np.arange(len(feature_label_names)))/len(feature_label_names)):\n",
    "    for jj, horizontal_pos in enumerate((0.5+np.arange(len(feature_label_names)+1))/(len(feature_label_names)+1)):\n",
    "        if normalized_confusion_matrix[ii, jj]>=0.5:\n",
    "            textcolor = 'k'\n",
    "        else:\n",
    "            textcolor = 'w'\n",
    "        ax2.text(horizontal_pos, vertical_pos, confusion_matrix[ii, jj], ha=\"center\", va=\"center\", color=textcolor)\n",
    "ax2.set_title(\"Confusion matrix; the numbers indicate amounts of datapoints\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929c239",
   "metadata": {},
   "source": [
    "### Same but with a multinomial logistic regression -model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to fit the model\n",
    "clf = LogisticRegression(random_state=0).fit(projected_data, feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities, classifications and classification uncertainty with the fitted model\n",
    "probabilities = np.transpose(clf.predict_proba(projected_data))\n",
    "classification_class = np.argmax(probabilities, axis=0)\n",
    "class_uncertainty = get_uncertainty_based_on_entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ae23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the class uncertainties as measured by the entropies, and also mark the maximum entropy \n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.hist(class_uncertainty[classification_class!=feature_labels], \n",
    "        bins = int(np.sqrt(np.sum(classification_class!=feature_labels))), \n",
    "        color='r', alpha=0.5, label='incorrect classifications');\n",
    "ax.hist(class_uncertainty[classification_class==feature_labels], \n",
    "        bins = int(np.sqrt(np.sum(classification_class==feature_labels))), \n",
    "        color='g', alpha=0.5, label='correct classifications');\n",
    "ylim = ax.get_ylim()\n",
    "ax.vlines(max_entropy, ylim[0], ylim[1], linestyles='dashed', colors=['k'], label='maximally entropic distribution')\n",
    "ax.legend(loc='center')\n",
    "ax.set_title('Entropies of predicted class probabilities');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292088dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# ----------> CHANGE CODE: VARY THIS\n",
    "entropy_threshold = np.min(class_uncertainty[classification_class!=feature_labels]) # \"cheat\"\n",
    "# <--------- CHANGE CODE: VARY THIS\n",
    "################################################################################################\n",
    "\n",
    "classification_class_adjusted = classification_class.copy()\n",
    "too_uncertain = class_uncertainty >= entropy_threshold \n",
    "classification_class_adjusted[too_uncertain]=num_subspecies\n",
    "\n",
    "# Scatter plotting to visualize classifications \n",
    "# ----------------------------------------------\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=[FIGSIZE[0], 2*FIGSIZE[1]])\n",
    "for subspecies_index, subspecies_label in enumerate(feature_label_names):\n",
    "    subspecies_rows = feature_labels == subspecies_index\n",
    "    num_subspecies_examples = np.sum(subspecies_rows)\n",
    "    ax1.scatter(projected_data[subspecies_rows, 0], projected_data[subspecies_rows, 1],\n",
    "               c = feature_label_marker_colors[subspecies_index:subspecies_index+1, :],\n",
    "               alpha=1-(class_uncertainty[subspecies_rows]/max_entropy), s=100, label=subspecies_label,\n",
    "               edgecolors = feature_label_marker_colors[classification_class_adjusted[subspecies_rows], :]);\n",
    "    subspecies_mean = np.mean(projected_data[subspecies_rows, :], axis=0, keepdims=True)\n",
    "    centered_subspecies_points = projected_data[subspecies_rows, :]-subspecies_mean\n",
    "    subspecies_covariance = np.dot(centered_subspecies_points.transpose(), \n",
    "                                   centered_subspecies_points)/(num_subspecies_examples-1)\n",
    "    equiprobability_points = get_2D_gaussian_equiprobability_points(subspecies_mean, subspecies_covariance)\n",
    "    ax1.plot(equiprobability_points[:, 0], equiprobability_points[:, 1], \n",
    "             color=feature_label_marker_colors[subspecies_index:subspecies_index+1, :], \n",
    "             alpha=0.5, linestyle='dashed', label=subspecies_label+'-gaussian')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('projection dimension 1'); ax.set_ylabel('projection dimension 2');\n",
    "ax1.set_title('Projected data classified: \\n colour indicates predicted class, \\n certainty in prediction encoded by colour strength, \\n black edges indicate too much uncertainty and thus refusal to classify');\n",
    "\n",
    "# Confusion matrix to visualize classifications\n",
    "# ----------------------------------------------\n",
    "\n",
    "# calculate confusion matrix\n",
    "confusion_matrix = np.empty((num_subspecies, num_subspecies+1), dtype='int')\n",
    "for true_index in np.arange(num_subspecies):\n",
    "    for classified_index in np.arange(num_subspecies+1):\n",
    "        confusion_matrix[true_index, classified_index] = np.sum(np.logical_and(feature_labels == true_index, classification_class_adjusted == classified_index))\n",
    "normalized_confusion_matrix = confusion_matrix/np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# Visualize confusion matrix as an image, gray-scale value encodes the numerical value\n",
    "im = ax2.imshow(normalized_confusion_matrix, cmap=plt.cm.gray, interpolation=\"nearest\", extent=[0, 1, 0, 1])\n",
    "ax2.tick_params(top=False, bottom=False, labeltop=True, labelbottom=False, left=False, labelleft=True, right=False)\n",
    "ax2.set_yticks((0.5+np.arange(len(feature_label_names)))/len(feature_label_names), feature_label_names[::-1])\n",
    "ax2.set_xticks((0.5+np.arange(len(feature_label_names)+1))/(len(feature_label_names)+1), \n",
    "               labels=['predicted as '+feature_label_name for feature_label_name in feature_label_names]+['too uncertain to classify'])\n",
    "\n",
    "# Append text annotations to show also the numerical values in textual format\n",
    "for ii, vertical_pos in enumerate(1-(0.5+np.arange(len(feature_label_names)))/len(feature_label_names)):\n",
    "    for jj, horizontal_pos in enumerate((0.5+np.arange(len(feature_label_names)+1))/(len(feature_label_names)+1)):\n",
    "        if normalized_confusion_matrix[ii, jj]>=0.5:\n",
    "            textcolor = 'k'\n",
    "        else:\n",
    "            textcolor = 'w'\n",
    "        ax2.text(horizontal_pos, vertical_pos, confusion_matrix[ii, jj], ha=\"center\", va=\"center\", color=textcolor)\n",
    "ax2.set_title(\"Confusion matrix; the numbers indicate amounts of datapoints\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae6a2e",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1e90b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244b3165",
   "metadata": {},
   "source": [
    "## <u> Task 5: Clustering data </u>\n",
    "Here we fit Gaussian mixture models (GMMs) to the projected data, and assess clustering that can be done \n",
    "by them. The probability density function (pdf) of a GMM, is a weighted sum of Gaussian pdfs with each Gaussian representing \n",
    "a mixture component, and the mixture component weights are non-negative, sum to one, and provide the probabilities of observing the components. The fitting of the model parameters (means, covariances and mixture weights of the component Gaussians) is done by uting the scikit-learn library.\n",
    "\n",
    "One a model is fitted, we here assign a data point to a component/cluster, by finding which \n",
    "of the components (say with index from 1 to K, under a GMM with K components) gives the highest posterior probability $p(Z=z|\\boldsymbol{X}=\\boldsymbol{x})$ which is proportional to $p(Z=z)p(\\boldsymbol{X}=\\boldsymbol{x}|Z=z)$, where\n",
    "$Z$ is a discrete random variable that indicates component index, $\\boldsymbol{X}$ is multivariate random variable that indicates the coordinates of the data point; $p(Z=z)$ is given by the mixture component weight of component $z$, and \n",
    "the $p(\\boldsymbol{X}=\\boldsymbol{x}|Z=z)$ is given by the pdf of the z-component -specific multivariate normal distribution.\n",
    "\n",
    "The codes below do the fitting as well as visualization of the clustering results. The clustering results are shown in the projected space (where the model fitting and inference happens), and also in the original space via seaborn.pairplot; in the projected space information about the component uncertainty is provided, numerically by one minus, the entropy of the distribution of $Z|\\boldsymbol{X}$ divided by the maximum possible entropy.\n",
    "\n",
    "Fit a gaussian mixture model with a varying amount of components, from 4 to 2; different runs may result in different results - sometimes there might be numerical issues due to the implementation, but don't worry about such. Assess if it is possible to get groups that effectively correspond to the subspecies-specific datapoints, robustly. \n",
    "\n",
    "In the 'Answer'-section, briefly, provide your assessment results, and comment on at least some potential benefits in doing the clustering in the projected (lower-dimensional) space vs. directly in the (higher-dimensional) original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01edcf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# ----------> CHANGE CODE: VARY THIS\n",
    "num_components = 4\n",
    "# <--------- CHANGE CODE: VARY THIS\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "# Fit a Gaussian mixture model\n",
    "# -----------------------------\n",
    "gmm = GaussianMixture(n_components=num_components).fit(projected_data)\n",
    "\n",
    "# Calculate component probabilities, assignments to components, and entropies, with the model\n",
    "# -------------------------------------------------------------------------------------------\n",
    "component_likelihoods = np.transpose(gmm.predict_proba(projected_data))\n",
    "unnormalized_posterior_probabilities = component_likelihoods*np.reshape(gmm.weights_, [num_components, 1])\n",
    "normalized_posterior_probabilities = unnormalized_posterior_probabilities/np.sum(unnormalized_posterior_probabilities, axis=0, keepdims=True)\n",
    "component_assignment = np.argmax(normalized_posterior_probabilities, axis=0)\n",
    "component_uncertainty = get_uncertainty_based_on_entropy(normalized_posterior_probabilities)\n",
    "\n",
    "# Scatter plotting to visualize classifications \n",
    "# ----------------------------------------------\n",
    "marker_colours = np.zeros((4, 3))\n",
    "marker_colours[0, :] = [1, 0, 0]\n",
    "marker_colours[1, :] = [0, 1, 0]\n",
    "marker_colours[2, :] = [0, 0, 1]\n",
    "marker_colours[3, :] = [0, 0, 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=FIGSIZE)\n",
    "for component_index in np.arange(num_components):\n",
    "    component_rows = component_assignment == component_index\n",
    "    num_component_examples = np.sum(component_rows)\n",
    "    ax.scatter(projected_data[component_rows, 0], projected_data[component_rows, 1],\n",
    "               c = marker_colours[component_index:component_index+1, :],\n",
    "               alpha=1-(component_uncertainty[component_rows]/max_entropy), s=100, label=f'component {component_index+1}',\n",
    "               edgecolors = marker_colours[component_assignment[component_rows], :]);\n",
    "    component_mean = gmm.means_[component_index, :].copy()\n",
    "    component_covariance = gmm.covariances_[component_index, :, :].copy()\n",
    "    equiprobability_points = get_2D_gaussian_equiprobability_points(component_mean, component_covariance)\n",
    "    ax.plot(equiprobability_points[:, 0], equiprobability_points[:, 1], \n",
    "             color=marker_colours[component_index:component_index+1, :], \n",
    "             alpha=0.5, linestyle='dashed', label=f'component {component_index+1} std from mean')\n",
    "ax.legend()\n",
    "ax.set_xlabel('projection dimension 1'); ax.set_ylabel('projection dimension 2');\n",
    "ax.set_title('Projected data clustered: \\n colour indicates most likely cluster/component, \\n certainty in component encoded by colour strength');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['component'] = component_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36abf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a pairplot by using the clustering information\n",
    "pairplot_original_clustered = sns.pairplot(df[feature_names+['component']], hue=\"component\", \n",
    "                                           palette=['r', 'g', 'b', 'k'][0: num_components], \n",
    "                                           diag_kind=\"kde\", diag_kws={'fill': False})\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle('Original data with colouring by the clustering');\n",
    "\n",
    "# compare to the original\n",
    "pairplot_original_clustered = sns.pairplot(df[feature_names+['subspecies']], hue=\"subspecies\", \n",
    "                                           palette=\"tab10\", diag_kind=\"kde\", diag_kws={'fill': False})\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle('Original data with colouring by the subspecies');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297568e8",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df74d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
