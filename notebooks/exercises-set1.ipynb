{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0135c0a3",
   "metadata": {},
   "source": [
    "# Exercise set 1\n",
    "Instructions: Use Google Colab to run the notebook; click the rocket-icon and choose Colab. Submit the completed notebook as instructed in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153fcf3",
   "metadata": {},
   "source": [
    "## Task 0: get and import needed helper code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b083545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/luma-lapinamk/jyri-pso23.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3298851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'jyri-pso23/code')\n",
    "from helpers_set1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba2be8",
   "metadata": {},
   "source": [
    "# Univariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30738a",
   "metadata": {},
   "source": [
    "## Task 1: Specifying a univariate Gaussian \n",
    "The code in the cell below creates plot of a probability density function (pdf) of a \n",
    "univariate Gaussian whose parameter values can be (re)set interactively.\n",
    "\n",
    "Play with setting the parameter values, and select parameter values that are different from the default ones. \n",
    "\n",
    "In the 'Answer'-section, briefly describe in the empty cell i) what do the parameters mean/control, ii) how does changing them can be seen from the plot, and iii) what parameter values did you choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc80f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = plot_and_specify_univariate_gaussian_interactively()\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeaef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, _, __ = interactive_plot.result # extracts the parameter values from the interactive plot instance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fbfe0",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28aadbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42beed67",
   "metadata": {},
   "source": [
    "## Task 2: Sampling from the distribution \n",
    "Now let's draw a bunch of samples from the distribution and look what kind of values did we get out.\n",
    "\n",
    "The following cell i) draws samples from the distribution, ii) calculates a normalized histogram of the samples (to get an approximate representation of the density of the sample values), iii) plots the distribution we are sampling from (in blue) and the obtained normalized histogram (in magenta with a bar plot and in cyan using a curve). \n",
    "\n",
    "Play with the values of the variables 'num_samples' (default is 10) and 'num_bins' (default is 10).  \n",
    "\n",
    "Change their values so that the match between the blue coloured curve (true density) and the cyan coloured curve (density representation of samples via a normalized histogram) is improved, ideally good.\n",
    "\n",
    "In the 'Answer'-section, briefly describe in the empty cell how did you change the parameter values and why did they result in a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "num_samples = 10 \n",
    "samples = mu+sigma*np.random.randn(num_samples)\n",
    "\n",
    "# get an approximate representation of the density \n",
    "num_bins = 10\n",
    "bin_centers, normalized_bin_counts = histogram_estimation_of_univariate_data_density(samples, n_bins=num_bins)\n",
    "\n",
    "# plot the true density and the histogram based approximate density\n",
    "_,__, fig, ax = plot_univariate_gaussian_pdf(mu, sigma)\n",
    "ax.plot(bin_centers, normalized_bin_counts, linestyle='dashed', color='cyan', linewidth=3, alpha=0.8)\n",
    "ax.bar(bin_centers, normalized_bin_counts, width=20./num_bins, color='magenta', \n",
    "       edgecolor='black' if num_bins<= 100 else 'magenta', alpha=0.5)\n",
    "ax.set_title('True density vs. normalized histogram of samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485f8be",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de620e53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c93a0b4a",
   "metadata": {},
   "source": [
    "## Task 3: Estimating true parameter values based on data samples\n",
    "Now let's estimate true parameter values of the distribution, from a set of samples from the true distribution. Let us do that for a varying amount of samples, and let us also assess estimation effectiveness for each case. Let us assess the effectiveness by comparing true and estimated parameter values, and the KL-divergence between the true and the estimated distribution.\n",
    "\n",
    "Codes for doing all of that is provided in the cell below. Your task is to replace\n",
    "the parameter estimators to be better ones, ideally the best ones; there are two lines\n",
    "of code that you should change, one starting with 'mu_estimate = ' and the other starting\n",
    "with 'sigma_estimate = ', the ones between the two lines containing comment 'CHANGE CODE'.\n",
    "\n",
    "In the 'Answer'-section, briefly describe i) your changes, ii) how and why did the results improve, and iii) the dependency on the estimation effectiveness to the amount of samples used when considering the improved approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1889482",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_list = [10**1, 10**2, 10**3, 10**4, 10**5]\n",
    "num_samples_cases = len(num_samples_list)\n",
    "mu_estimates = np.empty((num_samples_cases,), dtype=float)\n",
    "sigma_estimates = np.empty((num_samples_cases,), dtype=float)\n",
    "kl_divergences = np.empty((num_samples_cases,), dtype=float)\n",
    "\n",
    "# get samples, estimate parameters, assess effectiveness: \n",
    "# loop over the varying case of differing amount of samples\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "for num_samples_case_index, num_samples in enumerate(num_samples_list):\n",
    "    # sample from the distribution\n",
    "    samples = mu+sigma*np.random.randn(num_samples)\n",
    "    \n",
    "    # estimate parameter values from the distribution\n",
    "    ###########################################################################\n",
    "    # ----------> CHANGE CODE: IMPROVE THE ESTIMATORS, IF IMPROVING IS POSSIBLE\n",
    "    mu_estimate = (samples.min()+samples.max())/2 \n",
    "    sigma_estimate = (samples.max()-samples.min())/2 \n",
    "    # < --------- CHANGE CODE \n",
    "    ###########################################################################\n",
    "    \n",
    "    # calculate KL-divergence between true and estimated distribution\n",
    "    kl_div = kl_divergence_between_two_univariate_gaussians(mu, sigma, mu_estimate, sigma_estimate)\n",
    "    \n",
    "    # record the obtained parameter values and the KL-divergence\n",
    "    mu_estimates[num_samples_case_index] = mu_estimate\n",
    "    sigma_estimates[num_samples_case_index] = sigma_estimate\n",
    "    kl_divergences[num_samples_case_index] = kl_div\n",
    "    \n",
    "# plot results\n",
    "# -------------\n",
    "# create figure infrastructure\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=[12, 4], gridspec_kw={'height_ratios': [1]})\n",
    "\n",
    "# plot mu vs mu_estimated\n",
    "ax1.plot(np.array(num_samples_list), mu_estimates); xlim = ax1.get_xlim()\n",
    "ax1.hlines(mu, xlim[0], xlim[1], linestyle='dashed', colors=['b'], linewidth=4)\n",
    "ax1.set_title(r'$\\mu$ vs. $\\hat{\\mu}$'); ax1.set_xlabel('amount of samples')\n",
    "\n",
    "# plot sigma vs sigma_estimated\n",
    "ax2.plot(np.array(num_samples_list), sigma_estimates); xlim = ax2.get_xlim()\n",
    "ax2.hlines(sigma, xlim[0], xlim[1], linestyle='dashed', colors=['b'], linewidth=4)\n",
    "ax2.set_title(r'$\\sigma$ vs. $\\hat{\\sigma}$'); ax2.set_xlabel('amount of samples')\n",
    "\n",
    "# plot KL-divergence (p_true || p_estimated)\n",
    "ax3.plot(np.array(num_samples_list), kl_divergences); xlim = ax3.get_xlim()\n",
    "ax3.hlines(0, xlim[0], xlim[1], linestyle='dashed', colors=['b'], linewidth=4)\n",
    "ax3.set_title(r'$D_{KL}\\left(\\mathcal{N}(\\mu, \\sigma) \\|| \\mathcal{N}(\\hat{\\mu}, \\hat{\\sigma})\\right)$')\n",
    "ax3.set_xlabel('amount of samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d660d",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ec523",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8faed56c",
   "metadata": {},
   "source": [
    "# Multivariate data\n",
    "Similar as above, but for multivariate (bivariate) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e609ad0",
   "metadata": {},
   "source": [
    "## Task 4: Specifying a multivariate Gaussian \n",
    "The code in the cell below creates a plot of a probability density function (pdf) of a \n",
    "bivariate Gaussian whose parameter values can be (re)set interactively.\n",
    "\n",
    "Play with setting the parameter values, and select parameter values that are different from the default ones. \n",
    "\n",
    "In the 'Answer'-section, briefly describe in the empty cell i) what do the parameters mean/control, ii) how does changing them can be seen from the plot, and iii) what parameter values did you choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4342784",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = plot_and_specify_bivariate_gaussian_interactively()\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, K, _, __ = interactive_plot.result # extracts the parameter values from the interactive plot instance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee5b39",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b63bad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88d0e3ae",
   "metadata": {},
   "source": [
    "## Task 5: Sampling from the distribution and estimating true parameter values based on the samples\n",
    "Now let's draw a bunch of samples from the (chosen) distribution, and based on them, let's estimate the true parameter values of the distribution. Let us do that for a varying amount of samples, and let us also assess estimation effectiveness for each case. Let us assess the effectiveness by comparing the true and estimated parameter values, and the KL-divergence between the true and the estimated distribution.\n",
    "\n",
    "Codes for doing all of that is provided in the cell below. Your task is to replace\n",
    "the parameter estimators to be better ones, ideally the best ones; there are two lines\n",
    "of code that you should change, one starting with 'mu_estimate = ' and the other starting\n",
    "with 'K_estimate = ', the ones between the two lines containing comment 'CHANGE CODE'.\n",
    "\n",
    "In the 'Answer'-section, briefly describe i) your changes, ii) how and why did the results improve, and iii) the dependency on the estimation effectiveness to the amount of samples used when considering the improved approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d65965",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_list = [10**1, 10**2, 10**3, 10**4, 10**5]\n",
    "num_samples_cases = len(num_samples_list)\n",
    "mu_estimates = np.empty((num_samples_cases, 2), dtype=float)\n",
    "kl_divergences = np.empty((num_samples_cases,), dtype=float)\n",
    "\n",
    "# get samples, estimate parameters, assess effectiveness: \n",
    "# loop over the varying case of differing amount of samples\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "for num_samples_case_index, num_samples in enumerate(num_samples_list):\n",
    "    # sample from the distribution\n",
    "    samples = np.random.multivariate_normal(mean=mu.flatten(), cov=K, size=num_samples)\n",
    "    \n",
    "    # estimate parameter values from the distribution\n",
    "    ###########################################################################\n",
    "    # ----------> CHANGE CODE: IMPROVE THE ESTIMATORS, IF IMPROVING IS POSSIBLE\n",
    "    mu_estimate = 0.5*(np.min(samples, axis=0)+np.max(samples, axis=0)) \n",
    "    K_estimate = np.var(samples[:])*np.eye(2) # np.diag(np.var(samples, axis=0))\n",
    "    # < --------- CHANGE CODE \n",
    "    ###########################################################################\n",
    "    \n",
    "    # calculate KL-divergence between the true and estimated distribution\n",
    "    kl_div = kl_divergence_between_two_multivariate_gaussians(mu, K, mu_estimate, K_estimate)\n",
    "    \n",
    "    # record the obtained parameter values, the KL-divergence, and the normalized log-likelihood\n",
    "    mu_estimates[num_samples_case_index, :] = mu_estimate.copy()\n",
    "    kl_divergences[num_samples_case_index] = kl_div\n",
    "    \n",
    "# plot results\n",
    "# -------------\n",
    "\n",
    "# create figure infrastructure\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=[12, 4], gridspec_kw={'height_ratios': [1]})\n",
    "\n",
    "# plot mu vs mu_estimated\n",
    "ax1.plot(mu_estimates[:, 0], mu_estimates[:, 1], label='estimated') \n",
    "for num_samples_case_index, num_samples in enumerate(num_samples_list):\n",
    "    ax1.text(mu_estimates[num_samples_case_index, 0], mu_estimates[num_samples_case_index, 1], str(num_samples))\n",
    "ax1.plot(mu[0, 0], mu[0, 1], 'x', color='r', markersize=10, label='true')\n",
    "ax1.set_xlabel(r'$x_1$'); ax1.set_ylabel(r'$x_2$') \n",
    "ax1.set_title(r'$\\mu$ vs. $\\hat{\\mu}$'); ax1.legend()\n",
    "\n",
    "# plot KL-divergence (p_true || p_estimated)\n",
    "ax2.plot(np.array(num_samples_list), kl_divergences); xlim = ax2.get_xlim()\n",
    "ax2.hlines(0, xlim[0], xlim[1], linestyle='dashed', colors=['b'], linewidth=4)\n",
    "ax2.set_title(r'KL-divergence $D_{KL}\\left(\\mathcal{N}(\\mu, \\Sigma) \\|| \\mathcal{N}(\\hat{\\mu}, \\hat{\\Sigma})\\right)$')\n",
    "ax2.set_xlabel(r'amount of samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d34b3d",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Input your answers to the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c90b59",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
